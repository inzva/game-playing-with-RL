{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt1OjEjJz8hl"
   },
   "source": [
    "Thanks to https://medium.com/coinmonks/landing-a-rocket-with-simple-reinforcement-learning-3a0265f8b58c\n",
    "\n",
    "\n",
    "CEM Building Blocks\n",
    "To implement the deep cross entropy method we need to follow 4 steps\n",
    "\n",
    "1. Generate Sessions:\n",
    "Play through several episodes of the game environment with our current agent and save the actions, states and rewards used for each episode\n",
    "\n",
    "2. Retrieve Elite Sessions:\n",
    "We want to only learn off episodes that achieved a high score in that batch. We determine our elite threshold by taking some percentile of all the episode rewards for that batch of generated sessions.\n",
    "\n",
    "3. Train On Elite Sessions\n",
    "Now that we have the top X% of our batch, we train our model on those experiences. We use the recorded states as input and the actions carried out as the target output.\n",
    "\n",
    "4. Rinse and Repeat\n",
    "We keep repeating this process until our model converges on a successfull policy\n",
    "\n",
    "Now that we have our blueprint of how to build the agent the last thing we need to cover is applying deep learning to the CEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "kpIE1pkYLaE5",
    "outputId": "6dc87ac0-c53e-415a-cb53-e8c2e343fc59",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,obs_size,hidden_size,n_actions):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,n_actions)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "def generate_batch(env,batch_size,t_max = 1000):\n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states,batch_rewards = [],[],[]\n",
    "    for b in range(batch_size):\n",
    "        states,actions=[],[]   \n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probe_v = activation(net(s_v))\n",
    "            act_probs = act_probe_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs),p=act_probs)\n",
    "\n",
    "            new_s,r,done,info = env.step(a)\n",
    "\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "    return batch_states,batch_actions,batch_rewards\n",
    "\n",
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "    return elite_states,elite_actions\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "#Bununla oynayabilirsiniz\n",
    "session_size = 100\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "learning_rate = 0.0025\n",
    "completion_score = 200\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = generate_batch(env, batch_size, t_max=5000)\n",
    "    elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards),np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f,reward_threshold=%.1f\"% (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wokqoRisLZ80"
   },
   "outputs": [],
   "source": [
    "#Watch the intelligent agent.\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "generate_batch(env, 1, t_max=5000)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cem_lunarlander.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
